<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=220&section=header&text=ğŸ™ï¸%20Speech%20Emotion%20Recognition&fontSize=42&fontColor=fff&animation=twinkling&fontAlignY=35&desc=Real-Time%20Emotion%20Detection%20Powered%20by%20Bidirectional%20LSTM&descSize=18&descAlignY=55&descColor=fff" width="100%" />
</p>

<p align="center">
  <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=600&size=22&duration=3000&pause=1000&color=4A90D9&center=true&vCenter=true&multiline=true&repeat=false&width=800&height=80&lines=ğŸ§ +Deep+Learning+%7C+ğŸ¤+Audio+Processing+%7C+âš¡+Real-Time+Inference;Transforming+Speech+Into+Emotional+Intelligence" alt="Typing SVG" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Status-Active-success?style=for-the-badge&logo=statuspage&logoColor=white" alt="Status" />
  <img src="https://img.shields.io/badge/License-MIT-blue?style=for-the-badge&logo=opensourceinitiative&logoColor=white" alt="License" />
  <img src="https://img.shields.io/badge/PRs-Welcome-brightgreen?style=for-the-badge&logo=github&logoColor=white" alt="PRs Welcome" />
  <img src="https://img.shields.io/badge/Maintained-Yes-green?style=for-the-badge&logo=checkmarx&logoColor=white" alt="Maintained" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.9+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python" />
  <img src="https://img.shields.io/badge/TensorFlow-2.18-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white" alt="TensorFlow" />
  <img src="https://img.shields.io/badge/Flask-3.1-000000?style=for-the-badge&logo=flask&logoColor=white" alt="Flask" />
  <img src="https://img.shields.io/badge/React-19-61DAFB?style=for-the-badge&logo=react&logoColor=black" alt="React" />
  <img src="https://img.shields.io/badge/Vite-7.3-646CFF?style=for-the-badge&logo=vite&logoColor=white" alt="Vite" />
  <img src="https://img.shields.io/badge/Bootstrap-5.3-7952B3?style=for-the-badge&logo=bootstrap&logoColor=white" alt="Bootstrap" />
  <img src="https://img.shields.io/badge/librosa-0.10-4B8BBE?style=for-the-badge&logo=soundcharts&logoColor=white" alt="librosa" />
</p>

---

<p align="center">
  <a href="#-overview">Overview</a> â€¢
  <a href="#-features">Features</a> â€¢
  <a href="#-architecture">Architecture</a> â€¢
  <a href="#-screenshots--demo">Screenshots</a> â€¢
  <a href="#-quick-start">Quick Start</a> â€¢
  <a href="#-api-documentation">API Docs</a> â€¢
  <a href="#-technical-deep-dive">Deep Dive</a> â€¢
  <a href="#-tech-stack">Tech Stack</a> â€¢
  <a href="#-contributing">Contributing</a>
</p>

---

## ğŸ¯ Overview

<table>
<tr>
<td width="50%">

### ğŸ” What is this?

A **production-ready, full-stack web application** that classifies human speech into **four emotional states** â€” Angry, Happy, Sad, and Disgust â€” in real-time using a pre-trained **Bidirectional LSTM** deep learning model.

The system captures audio from your browser microphone, processes 3-second chunks through a sophisticated MFCC feature extraction pipeline, and delivers instant emotion predictions with confidence scores.

</td>
<td width="50%">

### ğŸ’¡ Why this project?

| Aspect | Detail |
|--------|--------|
| ğŸ§  **Problem** | Understanding emotional context in speech is critical for AI-human interaction |
| ğŸ¯ **Solution** | End-to-end ML pipeline from raw audio to real-time emotion classification |
| ğŸ—ï¸ **Engineering** | Clean separation of concerns: feature extraction â†’ inference â†’ API â†’ dashboard |
| ğŸ“Š **Dataset** | Trained on **CREMA-D** â€” a crowd-sourced multimodal emotion dataset |

</td>
</tr>
</table>

---

## âœ¨ Features

<table>
<tr>
<th>Feature</th>
<th>Description</th>
<th>Status</th>
</tr>
<tr>
<td>ğŸ¤ <b>Real-Time Mic Input</b></td>
<td>Browser MediaRecorder API captures 3-second audio chunks automatically</td>
<td>âœ…</td>
</tr>
<tr>
<td>ğŸ§  <b>Bi-LSTM Inference</b></td>
<td>Bidirectional LSTM model with Masking layer for variable-length audio</td>
<td>âœ…</td>
</tr>
<tr>
<td>ğŸ“Š <b>Live Probability Bars</b></td>
<td>Animated horizontal bars showing softmax probabilities for all 4 classes</td>
<td>âœ…</td>
</tr>
<tr>
<td>ğŸ“ˆ <b>Emotion Timeline</b></td>
<td>Scrollable history of last 10 predictions with timestamps and confidence</td>
<td>âœ…</td>
</tr>
<tr>
<td>ğŸ“‰ <b>Session Analytics</b></td>
<td>Live session duration, prediction count, and emotion distribution breakdown</td>
<td>âœ…</td>
</tr>
<tr>
<td>ğŸ¨ <b>Clean Modern UI</b></td>
<td>Minimal design with soft colors, rounded cards, subtle shadows, smooth transitions</td>
<td>âœ…</td>
</tr>
<tr>
<td>âš¡ <b>Async Processing</b></td>
<td>Non-blocking audio capture and API calls keep the UI responsive</td>
<td>âœ…</td>
</tr>
<tr>
<td>ğŸ›¡ï¸ <b>Error Handling</b></td>
<td>Input validation, structured logging, graceful error recovery</td>
<td>âœ…</td>
</tr>
<tr>
<td>ğŸ” <b>Continuous Recording</b></td>
<td>Auto-restart recording loop for uninterrupted live prediction</td>
<td>âœ…</td>
</tr>
<tr>
<td>ğŸ©º <b>Health Monitoring</b></td>
<td>Backend health-check endpoint for deployment readiness</td>
<td>âœ…</td>
</tr>
</table>

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              BROWSER CLIENT                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ MicRecorder  â”‚â”€â”€â–¶â”‚  Dashboard   â”‚â”€â”€â–¶â”‚ EmotionCard  â”‚  â”‚  Timeline    â”‚  â”‚
â”‚  â”‚ (3s chunks)  â”‚   â”‚  (State Mgr) â”‚   â”‚ (Display)    â”‚  â”‚  (History)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                  â”‚                                                â”‚
â”‚         â”‚    Audio Blob    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ Probability  â”‚  â”‚ SessionStats â”‚           â”‚
â”‚                               â”‚ Bars         â”‚  â”‚ (Analytics)  â”‚           â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ POST /predict (multipart/form-data)
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             FLASK API SERVER                               â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        app.py (Routes + CORS)                        â”‚   â”‚
â”‚  â”‚  POST /predict â”€â”€â–¶ Validate â”€â”€â–¶ Save Temp â”€â”€â–¶ Extract â”€â”€â–¶ Predict   â”‚   â”‚
â”‚  â”‚  GET  /health  â”€â”€â–¶ Status Check                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                           â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚     feature_extractor.py        â”‚  â”‚       inference.py              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ 1. Load @ 16kHz           â”‚ â”‚  â”‚  â”‚ EmotionPredictor          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ 2. Trim silence           â”‚ â”‚  â”‚  â”‚  â”œâ”€ Load Keras model      â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ 3. 40 MFCCs               â”‚ â”‚  â”‚  â”‚  â”œâ”€ Load StandardScaler   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ 4. Delta + DeltaÂ²         â”‚ â”‚  â”‚  â”‚  â”œâ”€ Scale features        â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ 5. Stack â†’ 120 features   â”‚ â”‚  â”‚  â”‚  â”œâ”€ Reshape (1,400,120)   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ 6. Pad/Truncate â†’ 400     â”‚ â”‚  â”‚  â”‚  â””â”€ model.predict()      â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    model/ (Artifacts)                                 â”‚   â”‚
â”‚  â”‚     final_ser_model.keras          scaler.pkl                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¸ Screenshots & Demo

<p align="center">
  <img src="https://img.shields.io/badge/ğŸ“±_Live_Application_Screenshots-4A90D9?style=for-the-badge" alt="Screenshots" />
</p>

### ğŸ  Dashboard â€” Initial State
> Clean landing page with Start Listening button, session analytics panel, and empty emotion display.

<p align="center">
  <img src="screenshots/2.png" alt="Dashboard Initial State" width="95%" />
</p>

### ğŸ”» Emotion Probabilities & Analytics Panel
> All four emotion probability bars and the session analytics breakdown displayed in a clean layout.

<p align="center">
  <img src="screenshots/3.png" alt="Emotion Probabilities Panel" width="95%" />
</p>

### ğŸ¯ Live Prediction â€” Angry Emotion Detected
> Real-time detection showing "Angry" with 67.3% confidence, alongside the emotion timeline showing prediction history.

<p align="center">
  <img src="screenshots/4.png" alt="Live Prediction - Angry Detected" width="95%" />
</p>

### ğŸ“Š Probability Bars & Session Distribution
> Detailed probability breakdown (Angry: 67.3%, Happy: 26.8%) with session analytics showing emotion distribution.

<p align="center">
  <img src="screenshots/5.png" alt="Probability Bars and Session Stats" width="95%" />
</p>

### ğŸ“ˆ Extended Session â€” 23 Predictions
> After a longer session: 88.8% confidence on Angry detection, 23 total predictions tracked, full emotion distribution analytics.

<p align="center">
  <img src="screenshots/1.png" alt="Extended Session Analytics" width="95%" />
</p>

---

## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites

| Requirement | Version | Check Command |
|-------------|---------|---------------|
| ğŸ Python | 3.9+ | `python --version` |
| ğŸ“¦ Node.js | 18+ | `node --version` |
| ğŸ“¦ npm | 9+ | `npm --version` |
| ğŸ”§ pip | Latest | `pip --version` |

### âš™ï¸ Installation

<details>
<summary><b>ğŸ“¦ Step 1 â€” Clone the Repository</b></summary>

```bash
git clone https://github.com/yourusername/speech-emotion-recognition.git
cd speech-emotion-recognition
```

</details>

<details>
<summary><b>ğŸ Step 2 â€” Set Up Backend</b></summary>

```bash
# Navigate to backend
cd backend

# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Linux/Mac)
# source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

</details>

<details>
<summary><b>âš›ï¸ Step 3 â€” Set Up Frontend</b></summary>

```bash
# Navigate to frontend
cd frontend

# Install dependencies
npm install
```

</details>

<details>
<summary><b>ğŸš€ Step 4 â€” Launch Application</b></summary>

```bash
# Terminal 1 â€” Start Backend (from backend/)
python app.py
# âœ… API running at http://localhost:5000

# Terminal 2 â€” Start Frontend (from frontend/)
npm run dev
# âœ… App running at http://localhost:5173
```

Now open **http://localhost:5173** â†’ Click **Start Listening** â†’ Grant mic permission â†’ Speak!

</details>

---

## ğŸ“¡ API Documentation

### `POST /predict` â€” Emotion Prediction

<table>
<tr><td><b>Method</b></td><td><code>POST</code></td></tr>
<tr><td><b>URL</b></td><td><code>http://localhost:5000/predict</code></td></tr>
<tr><td><b>Content-Type</b></td><td><code>multipart/form-data</code></td></tr>
<tr><td><b>Body</b></td><td><code>file</code>: Audio file (WAV, WebM, MP3, OGG, FLAC, M4A)</td></tr>
<tr><td><b>Max Size</b></td><td>10 MB</td></tr>
</table>

**âœ… Success Response (200):**

```json
{
  "emotion": "Happy",
  "confidence": 0.8423,
  "probabilities": {
    "Angry": 0.0512,
    "Happy": 0.8423,
    "Sad": 0.0601,
    "Disgust": 0.0464
  },
  "timestamp": "2026-02-17T15:45:00.000000"
}
```

**âŒ Error Responses:**

| Status | Error | Description |
|--------|-------|-------------|
| `400` | No audio file provided | Missing `file` field |
| `400` | Unsupported audio format | Invalid file extension |
| `422` | Audio too short | Less than 0.1s after trimming |
| `500` | Internal server error | Processing failure |

### `GET /health` â€” Health Check

```json
{
  "status": "healthy",
  "model_loaded": true,
  "timestamp": "2026-02-17T15:45:00.000000"
}
```

---

## ğŸ”¬ Technical Deep Dive

<details>
<summary><b>ğŸµ Feature Extraction Pipeline (Click to Expand)</b></summary>

<br/>

The audio preprocessing pipeline exactly mirrors the training notebook to ensure consistent predictions:

```
Raw Audio (Any SR) â”€â”€â–¶ Resample to 16kHz â”€â”€â–¶ Trim Silence
                                                    â”‚
                                                    â–¼
                                            Extract 40 MFCCs
                                         (n_fft=400, hop=160)
                                                    â”‚
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â–¼           â–¼           â–¼
                                      MFCC       Delta       DeltaÂ²
                                     (40)        (40)        (40)
                                        â”‚           â”‚           â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â–¼
                                          Stack â†’ (T, 120)
                                                    â”‚
                                                    â–¼
                                        Pad/Truncate â†’ (400, 120)
                                                    â”‚
                                                    â–¼
                                          StandardScaler Transform
                                                    â”‚
                                                    â–¼
                                        Reshape â†’ (1, 400, 120)
                                                    â”‚
                                                    â–¼
                                          Model Input Ready âœ…
```

**Key Parameters:**

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| `SAMPLE_RATE` | 16,000 Hz | Standard for speech processing |
| `N_MFCC` | 40 | Rich spectral representation |
| `N_FFT` | 400 | 25ms window at 16kHz |
| `HOP_LENGTH` | 160 | 10ms hop = 100 frames/sec |
| `MAX_FRAMES` | 400 | ~4 seconds of audio |
| `N_FEATURES` | 120 | 40 MFCC + 40 Î” + 40 Î”Â² |

</details>

<details>
<summary><b>ğŸ§  Model Architecture (Click to Expand)</b></summary>

<br/>

```
Input Shape: (batch, 400, 120)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Masking (value=0.0)  â”‚  â”€â”€ Handles padded sequences
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bidirectional LSTM   â”‚  â”€â”€ 128 units, return_sequences=True
â”‚  (256 outputs)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dropout (0.4)        â”‚  â”€â”€ Regularization
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bidirectional LSTM   â”‚  â”€â”€ 64 units, return_sequences=False
â”‚  (128 outputs)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dropout (0.4)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dense (64, ReLU)     â”‚  â”€â”€ Feature compression
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dropout (0.3)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dense (4, Softmax)   â”‚  â”€â”€ [Angry, Happy, Sad, Disgust]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Training Configuration:**

| Setting | Value |
|---------|-------|
| Optimizer | Adam (lr=1e-3) |
| Loss | Categorical Cross-Entropy |
| Callbacks | EarlyStopping (patience=8), ReduceLROnPlateau (patience=4) |
| Batch Size | 32 |
| Max Epochs | 40 |
| Dataset | CREMA-D (4 emotion classes) |

</details>

<details>
<summary><b>ğŸ”„ Real-Time Recording Flow (Click to Expand)</b></summary>

<br/>

```
User clicks "Start Listening"
        â”‚
        â–¼
Request mic permission â”€â”€â–¶ getUserMedia({ audio: { sampleRate: 16000 } })
        â”‚
        â–¼
Create MediaRecorder (audio/webm;codecs=opus)
        â”‚
        â”œâ”€â”€â–¶ Start recording
        â”‚         â”‚
        â”‚    3 seconds later
        â”‚         â”‚
        â”‚         â–¼
        â”‚    recorder.stop() â”€â”€â–¶ Blob created â”€â”€â–¶ POST /predict
        â”‚                                              â”‚
        â”‚                                         JSON response
        â”‚                                              â”‚
        â”‚                                              â–¼
        â”‚                                    Update Dashboard State
        â”‚                                    (emotion, probs, history)
        â”‚
        â””â”€â”€â–¶ Restart recording (loop every 3.2s)
```

**Anti-Flood Protection:** Frontend skips new recordings while a prediction request is in-flight.

</details>

---

## ğŸ“ Project Structure

```
speech-emotion-app/
â”‚
â”œâ”€â”€ ğŸ backend/                     # Flask API Server
â”‚   â”œâ”€â”€ app.py                      # Main server â€” routes, CORS, validation
â”‚   â”œâ”€â”€ requirements.txt            # Python dependencies
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ feature_extractor.py    # Audio â†’ MFCC feature pipeline
â”‚       â””â”€â”€ inference.py            # EmotionPredictor class
â”‚
â”œâ”€â”€ âš›ï¸ frontend/                     # React + Vite Client
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.jsx                 # Root component
â”‚       â”œâ”€â”€ main.jsx                # React entry point
â”‚       â”œâ”€â”€ styles.css              # Global design system
â”‚       â”œâ”€â”€ components/
â”‚       â”‚   â”œâ”€â”€ EmotionCard.jsx     # ğŸ˜ ğŸ˜ŠğŸ˜¢ğŸ¤¢ Detected emotion display
â”‚       â”‚   â”œâ”€â”€ ProbabilityBars.jsx # Animated horizontal bars
â”‚       â”‚   â”œâ”€â”€ Timeline.jsx        # Prediction history list
â”‚       â”‚   â”œâ”€â”€ MicRecorder.jsx     # Browser mic + MediaRecorder
â”‚       â”‚   â””â”€â”€ SessionStats.jsx    # Session duration + analytics
â”‚       â”œâ”€â”€ pages/
â”‚       â”‚   â””â”€â”€ Dashboard.jsx       # Main layout + state orchestration
â”‚       â””â”€â”€ services/
â”‚           â””â”€â”€ apiService.js       # Axios wrapper for API calls
â”‚
â”œâ”€â”€ ğŸ§  model/                       # ML Artifacts
â”‚   â”œâ”€â”€ final_ser_model.keras       # Trained Bi-LSTM model (~3.5 MB)
â”‚   â””â”€â”€ scaler.pkl                  # Fitted StandardScaler
â”‚
â”œâ”€â”€ ğŸ““ notebook/                     # Training Notebook
â”‚   â”œâ”€â”€ Speech_Emotion_Recognition.ipynb
â”‚   â””â”€â”€ speech_emotion_recognition.py
â”‚
â”œâ”€â”€ ğŸ“¸ screenshots/                  # Application screenshots
â”‚   â”œâ”€â”€ 1.png
â”‚   â”œâ”€â”€ 2.png
â”‚   â”œâ”€â”€ 3.png
â”‚   â”œâ”€â”€ 4.png
â”‚   â””â”€â”€ 5.png
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                       # You are here ğŸ“
```

---

## ğŸ› ï¸ Tech Stack

<table>
<tr>
<th align="center">Layer</th>
<th align="center">Technology</th>
<th align="center">Purpose</th>
</tr>
<tr>
<td align="center"><b>ğŸ¨ Frontend</b></td>
<td>
  <img src="https://img.shields.io/badge/React-61DAFB?style=flat-square&logo=react&logoColor=black" />
  <img src="https://img.shields.io/badge/Vite-646CFF?style=flat-square&logo=vite&logoColor=white" />
  <img src="https://img.shields.io/badge/Bootstrap-7952B3?style=flat-square&logo=bootstrap&logoColor=white" />
</td>
<td>Interactive dashboard, responsive UI, component library</td>
</tr>
<tr>
<td align="center"><b>ğŸ”— API Layer</b></td>
<td>
  <img src="https://img.shields.io/badge/Axios-5A29E4?style=flat-square&logo=axios&logoColor=white" />
</td>
<td>HTTP client for audio upload and health checks</td>
</tr>
<tr>
<td align="center"><b>âš™ï¸ Backend</b></td>
<td>
  <img src="https://img.shields.io/badge/Flask-000000?style=flat-square&logo=flask&logoColor=white" />
  <img src="https://img.shields.io/badge/Flask_CORS-000000?style=flat-square&logo=flask&logoColor=white" />
</td>
<td>REST API server with CORS, validation, structured logging</td>
</tr>
<tr>
<td align="center"><b>ğŸ§  ML / DL</b></td>
<td>
  <img src="https://img.shields.io/badge/TensorFlow-FF6F00?style=flat-square&logo=tensorflow&logoColor=white" />
  <img src="https://img.shields.io/badge/Keras-D00000?style=flat-square&logo=keras&logoColor=white" />
</td>
<td>Bidirectional LSTM model loading and inference</td>
</tr>
<tr>
<td align="center"><b>ğŸµ Audio</b></td>
<td>
  <img src="https://img.shields.io/badge/librosa-4B8BBE?style=flat-square&logo=soundcharts&logoColor=white" />
  <img src="https://img.shields.io/badge/SoundFile-336791?style=flat-square&logo=audio&logoColor=white" />
</td>
<td>MFCC extraction, delta features, audio I/O</td>
</tr>
<tr>
<td align="center"><b>ğŸ“ Preprocessing</b></td>
<td>
  <img src="https://img.shields.io/badge/scikit--learn-F7931E?style=flat-square&logo=scikitlearn&logoColor=white" />
  <img src="https://img.shields.io/badge/NumPy-013243?style=flat-square&logo=numpy&logoColor=white" />
</td>
<td>StandardScaler normalization, array operations</td>
</tr>
</table>

---

## ğŸ“Š Performance Metrics

| Metric | Value |
|--------|-------|
| ğŸ¯ **Model Input Shape** | `(1, 400, 120)` |
| ğŸ·ï¸ **Output Classes** | 4 (Angry, Happy, Sad, Disgust) |
| â±ï¸ **Recording Chunk** | 3 seconds |
| ğŸ”„ **Prediction Latency** | ~1-2 seconds (CPU) |
| ğŸ“¦ **Model Size** | ~3.5 MB (.keras) |
| ğŸ¤ **Audio Formats** | WAV, WebM, MP3, OGG, FLAC, M4A |
| ğŸ“ **Max Upload** | 10 MB |
| ğŸµ **Sample Rate** | 16,000 Hz |

---

## ğŸ—ºï¸ Roadmap

```mermaid
graph LR
    A["âœ… Core App"] --> B["âœ… Real-Time Mic"]
    B --> C["âœ… Live Dashboard"]
    C --> D["ğŸ”œ Docker Deploy"]
    D --> E["ğŸ”œ WebSocket Streaming"]
    E --> F["ğŸ”® Multi-Language"]
    F --> G["ğŸ”® Emotion+Sentiment Fusion"]

    style A fill:#27ae60,stroke:#1e8449,color:#fff
    style B fill:#27ae60,stroke:#1e8449,color:#fff
    style C fill:#27ae60,stroke:#1e8449,color:#fff
    style D fill:#f39c12,stroke:#d68910,color:#fff
    style E fill:#f39c12,stroke:#d68910,color:#fff
    style F fill:#8e44ad,stroke:#6c3483,color:#fff
    style G fill:#8e44ad,stroke:#6c3483,color:#fff
```

| Phase | Feature | Status |
|-------|---------|--------|
| 1ï¸âƒ£ | Core Bi-LSTM model training + evaluation | âœ… Done |
| 2ï¸âƒ£ | Flask REST API with feature extraction pipeline | âœ… Done |
| 3ï¸âƒ£ | React dashboard with real-time mic recording | âœ… Done |
| 4ï¸âƒ£ | Session analytics & emotion timeline | âœ… Done |
| 5ï¸âƒ£ | Docker containerization for deployment | ğŸ”œ Planned |
| 6ï¸âƒ£ | WebSocket-based streaming (lower latency) | ğŸ”œ Planned |
| 7ï¸âƒ£ | Multi-language emotion recognition | ğŸ”® Future |
| 8ï¸âƒ£ | Combined emotion + sentiment analysis | ğŸ”® Future |

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how:

```bash
# 1. Fork the repository
# 2. Create your feature branch
git checkout -b feature/amazing-feature

# 3. Commit your changes
git commit -m "feat: add amazing feature"

# 4. Push to the branch
git push origin feature/amazing-feature

# 5. Open a Pull Request
```

> ğŸ’¡ Please ensure your code follows existing patterns and includes appropriate error handling.

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¨â€ğŸ’» Author

<p align="center">
  <img src="https://img.shields.io/badge/Built%20with%20â¤ï¸%20by-Dinraj-4A90D9?style=for-the-badge" alt="Author" />
</p>

<p align="center">
  <a href="https://github.com/dinraj910">
    <img src="https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white" alt="GitHub" />
  </a>
  <a href="https://linkedin.com/in/dinraj910">
    <img src="https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn" />
  </a>
  <a href="mailto:dinraj910@gmail.com">
    <img src="https://img.shields.io/badge/Email-EA4335?style=for-the-badge&logo=gmail&logoColor=white" alt="Email" />
  </a>
</p>

---

## ğŸ™ Acknowledgments

- ğŸ­ [CREMA-D Dataset](https://github.com/CheyneyComputerScience/CREMA-D) â€” Crowd-sourced Emotional Multimodal Actors Dataset
- ğŸµ [librosa](https://librosa.org/) â€” Audio and music signal analysis in Python
- ğŸ§  [TensorFlow / Keras](https://www.tensorflow.org/) â€” Open-source machine learning framework
- ğŸ¨ [Bootstrap](https://getbootstrap.com/) â€” CSS component library
- âš¡ [Vite](https://vitejs.dev/) â€” Next-generation frontend tooling

---

<p align="center">
  <img src="https://img.shields.io/badge/â­_Star_this_repo_if_you_found_it_useful!-FFD700?style=for-the-badge" alt="Star" />
</p>

<p align="center">
  <b>If this project helped you, please consider giving it a â­</b>
</p>

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=120&section=footer" width="100%" />
</p>
