# ğŸ™ï¸ Speech Emotion Recognition

> Real-time speech emotion detection using a Bidirectional LSTM model, served via a Flask API with a React frontend.

![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.18-orange?logo=tensorflow&logoColor=white)
![React](https://img.shields.io/badge/React-19-61DAFB?logo=react&logoColor=black)
![Flask](https://img.shields.io/badge/Flask-3.1-black?logo=flask&logoColor=white)

---

## ğŸ“‹ Overview

This application classifies speech into **four emotions** â€” Angry, Happy, Sad, Disgust â€” using a pre-trained Bidirectional LSTM model trained on the **CREMA-D** dataset.

The system captures audio from the browser microphone in real-time, sends 3-second chunks to a Flask backend for feature extraction and inference, and displays results in an interactive dashboard.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Audio (WebM)    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React UI   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Flask API   â”‚
â”‚   (Vite)     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  (Python)    â”‚
â”‚              â”‚    JSON Response   â”‚              â”‚
â”‚  Dashboard   â”‚                    â”‚  /predict    â”‚
â”‚  Components  â”‚                    â”‚  /health     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚   Pipeline   â”‚
                                   â”‚  1. librosa  â”‚
                                   â”‚  2. Scaler   â”‚
                                   â”‚  3. Bi-LSTM  â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ Feature Extraction Pipeline

The audio preprocessing exactly mirrors the training pipeline:

| Step | Detail |
|------|--------|
| Resample | 16,000 Hz |
| Trim | Remove leading/trailing silence |
| MFCC | 40 coefficients, n_fft=400, hop=160 |
| Delta | 1st-order derivative of MFCCs |
| DeltaÂ² | 2nd-order derivative of MFCCs |
| Stack | 40 + 40 + 40 = **120 features** |
| Sequence | Pad/truncate to **400 frames** |
| Scale | StandardScaler (fitted on training data) |
| Input Shape | `(1, 400, 120)` |

---

## ğŸ§  Model Summary

- **Type**: Bidirectional LSTM (Keras)
- **Architecture**: Masking â†’ BiLSTM(128) â†’ Dropout â†’ BiLSTM(64) â†’ Dropout â†’ Dense(64, ReLU) â†’ Dense(4, Softmax)
- **Training Data**: CREMA-D (filtered to 4 emotions)
- **Output**: Softmax probabilities for [Angry, Happy, Sad, Disgust]

---

## ğŸ“¡ API Documentation

### `POST /predict`

Upload an audio file for emotion prediction.

**Request**: `multipart/form-data` with `file` field

**Response**:
```json
{
  "emotion": "Happy",
  "confidence": 0.8423,
  "probabilities": {
    "Angry": 0.0512,
    "Happy": 0.8423,
    "Sad": 0.0601,
    "Disgust": 0.0464
  },
  "timestamp": "2026-02-17T15:45:00.000000"
}
```

### `GET /health`

Health check endpoint.

---

## ğŸ“ Project Structure

```
speech-emotion-app/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                  # Flask API server
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ feature_extractor.py  # Audio â†’ MFCC features
â”‚       â””â”€â”€ inference.py          # Model prediction service
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ EmotionCard.jsx     # Detected emotion display
â”‚   â”‚   â”‚   â”œâ”€â”€ ProbabilityBars.jsx # Probability visualization
â”‚   â”‚   â”‚   â”œâ”€â”€ Timeline.jsx        # Prediction history
â”‚   â”‚   â”‚   â”œâ”€â”€ MicRecorder.jsx     # Browser mic recording
â”‚   â”‚   â”‚   â””â”€â”€ SessionStats.jsx    # Session analytics
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â””â”€â”€ Dashboard.jsx       # Main dashboard layout
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ apiService.js       # Axios API wrapper
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â””â”€â”€ styles.css
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ final_ser_model.keras       # Trained Bi-LSTM model
â”‚   â””â”€â”€ scaler.pkl                  # Fitted StandardScaler
â””â”€â”€ README.md
```

---

## ğŸš€ Setup Instructions

### Prerequisites

- Python 3.9+
- Node.js 18+
- npm 9+

### Backend

```bash
cd backend

# Create virtual environment
python -m venv venv
venv\Scripts\activate        # Windows
# source venv/bin/activate   # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Start server
python app.py
# â†’ API running at http://localhost:5000
```

### Frontend

```bash
cd frontend

# Install dependencies
npm install

# Start dev server
npm run dev
# â†’ App running at http://localhost:5173
```

---

## ğŸ–¼ï¸ Screenshots

<!-- Add screenshots here -->

| Dashboard | Live Prediction |
|-----------|-----------------|
| *Coming soon* | *Coming soon* |

---

## ğŸ³ Docker (Optional)

```dockerfile
# Backend
FROM python:3.9-slim
WORKDIR /app
COPY backend/ .
COPY model/ /app/model/
RUN pip install -r requirements.txt
EXPOSE 5000
CMD ["python", "app.py"]
```

---

## ğŸ“ License

This project is for educational and portfolio purposes.

---

## ğŸ™ Acknowledgments

- [CREMA-D Dataset](https://github.com/CheyneyComputerScience/CREMA-D) â€” Crowd-sourced Emotional Multimodal Actors Dataset
- [librosa](https://librosa.org/) â€” Audio feature extraction
- [TensorFlow/Keras](https://www.tensorflow.org/) â€” Deep learning framework
